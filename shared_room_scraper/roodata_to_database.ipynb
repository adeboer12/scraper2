{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Shared Room Scraper\n",
    "This notebook implements a variation on the scraper2.py that scrapes shared listings rather than apartments/hohusing rental listings. This is a project of the Urban Analytics Lab at UC Berkeley. In order to run this file, the user needs to define a private 'settings.json' file in the directory of the code. This file contains the credentials for the database host, name, and password."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import required packages\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "import importlib\n",
    "import urllib\n",
    "import unicodecsv as csv\n",
    "from lxml import html\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "from requests.auth import HTTPProxyAuth\n",
    "import time\n",
    "#import psycopg2\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x) #describe() vars are not in scientific notation\n",
    "pd.set_option('max_columns', 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'settings.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-441083761ebd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Read in credentials from private settings file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'settings.json'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msettings_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0msettings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'settings.json'"
     ]
    }
   ],
   "source": [
    "# Read in credentials from private settings file\n",
    "with open('settings.json') as settings_file:    \n",
    "    settings = json.load(settings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#charity, proxy, s, sessions variables are associated with the charity engine\n",
    "#Try to run this with San Francisco\n",
    "DOMAINS = ['http://sfbay.craigslist.org/search/roo'] \n",
    "\n",
    "#Craigslist doesn't use time zones in its timestamps, so these cutoffs will be\n",
    "#interpreted relative to the local time at the listing location. For example, dt.now()\n",
    "#run from a machine in San Francisco will match listings from 3 hours ago in Boston.\n",
    "LATEST_TS = dt.now() - timedelta(hours=1)  #lagged lookback to account for the delay we're getting in our results\n",
    "EARLIEST_TS = LATEST_TS - timedelta(hours=2)\n",
    "\n",
    "\n",
    "OUT_DIR =\"C:\\\\Users\\\\james\\\\Documents\\\\Berkeley_Docs\\\\Spring_17_Courses\\\\CP290 Data Lab\\\\scraper_output\\\\\" #James's directory\n",
    "#OUT_DIR =\"C:\\\\Users\\\\varun\\\\Documents\\\\Berkeley\\\\2017 Spring\\\\Workshop\\\\Datasets\\\\data_output\\\\\" #Varun's directory\n",
    "#OUT_DIR ='/Users/anniedbr/Desktop/CSV/'  #Annie's directory\n",
    "#OUT_DIR ='../../Scraped Listings/'  #Brian's directory\n",
    "\n",
    "FNAME_BASE = 'data'  # filename prefix for saved data\n",
    "FNAME_TS = True  # append timestamp to filename\n",
    "\n",
    "S3_UPLOAD = False\n",
    "S3_BUCKET = 'scraper2'\n",
    "\n",
    "class RentalListingScraper(object):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            domains = DOMAINS,\n",
    "            earliest_ts = EARLIEST_TS,\n",
    "            latest_ts = LATEST_TS, \n",
    "            out_dir = OUT_DIR,\n",
    "            fname_base = FNAME_BASE,\n",
    "            fname_ts = FNAME_TS,\n",
    "            s3_upload = S3_UPLOAD,\n",
    "            s3_bucket = S3_BUCKET):\n",
    "        \n",
    "        self.domains = domains\n",
    "        self.earliest_ts = earliest_ts\n",
    "        self.latest_ts = latest_ts\n",
    "        self.out_dir = out_dir\n",
    "        self.fname_base = fname_base\n",
    "        self.fname_ts = fname_ts\n",
    "        self.s3_upload = s3_upload\n",
    "        self.s3_bucket = s3_bucket\n",
    "        self.ts = dt.now().strftime('%Y%m%d-%H%M%S')  # Use timestamp as file id\n",
    "        #self.ts = fname_ts\n",
    "\n",
    "        log_fname = self.out_dir + self.fname_base \\\n",
    "                + (self.ts if self.fname_ts else '') + '.log'\n",
    "        \n",
    "        importlib.reload(logging)\n",
    "        \n",
    "        logging.basicConfig(filename=log_fname, level=logging.INFO)\n",
    "       \n",
    "\n",
    "        \n",
    "    def _get_str(self, list):\n",
    "        '''\n",
    "        The xpath() function returns a list of items that may be empty. Most of the time,\n",
    "        we want the first of any strings that match the xml query. This helper function\n",
    "        returns that string, or null if the list is empty.\n",
    "        '''\n",
    "        \n",
    "        if len(list) > 0:\n",
    "            return list[0]\n",
    "\n",
    "        return ''\n",
    "    \n",
    "        \n",
    "    def _get_int_prefix(self, str, label):\n",
    "        '''\n",
    "        Bedrooms and square footage have the format \"xx 1br xx 450ft xx\". This helper \n",
    "        function extracts relevant integers from strings of this format.\n",
    "        '''     \n",
    "        \n",
    "        for s in str.split(' '):\n",
    "            if label in s:\n",
    "                return s.strip(label)\n",
    "                \n",
    "        return 0\n",
    "\n",
    "\n",
    "    def _toFloat(self, string_value):\n",
    "        string_value = string_value.strip()\n",
    "        return np.float(string_value) if string_value else np.nan\n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "    def _parseListing(self, item):\n",
    "        '''\n",
    "        Note that xpath() returns a list with elements of varying types depending on the\n",
    "        query results: xml objects, strings, etc.\n",
    "        '''\n",
    "        pid = item.xpath('@data-pid')[0]  # post id, always present\n",
    "        info = item.xpath('p[@class=\"result-info\"]')[0]\n",
    "        dt = info.xpath('time/@datetime')[0]\n",
    "        url = info.xpath('a/@href')[0]\n",
    "        if type(info.xpath('a/text()')) == str:\n",
    "            title = info.xpath('a/text()')\n",
    "        else:\n",
    "            title = info.xpath('a/text()')[0]\n",
    "        price = self._get_str(info.xpath('span[@class=\"result-meta\"]/span[@class=\"result-price\"]/text()')).strip('$')\n",
    "        neighb_raw = info.xpath('span[@class=\"result-meta\"]/span[@class=\"result-hood\"]/text()')\n",
    "        if len(neighb_raw) == 0:\n",
    "            neighb = ''\n",
    "        else:\n",
    "            neighb = neighb_raw[0].strip(\" \").strip(\"(\").strip(\")\")\n",
    "        housing_raw = info.xpath('span[@class=\"result-meta\"]/span[@class=\"housing\"]/text()')\n",
    "        if len(housing_raw) == 0:\n",
    "            #beds = 0\n",
    "            sqft = 0\n",
    "        else:\n",
    "            bedsqft = housing_raw[0]\n",
    "            sqft = self._get_int_prefix(bedsqft, \"ft\")  # appears as \"000ft\" or missing\n",
    "\n",
    "        return [pid, dt, url, title, price, neighb, sqft]\n",
    "\n",
    "    \n",
    "\n",
    "    def PageBodyText(self, session, url, proxy=True):\n",
    "        #this grabs the entire XML structured text from each post, then cleans it a bit.  \n",
    "        \n",
    "        s = session\n",
    "        #page = requests.get(url)        \n",
    "        page = s.get(url, timeout=30, verify=False)\n",
    "        tree = html.fromstring(page.content)\n",
    "        path = tree.xpath('//section[@id=\"postingbody\"]')[0]\n",
    "               \n",
    "        body_list = path.xpath('text()')\n",
    "        \n",
    "        body_text = ''.join(body_list).strip().encode('utf-8')\n",
    "        \n",
    "        return [body_text]\n",
    "     \n",
    "    def _scrapeLatLng(self, session, url, proxy=True):\n",
    "    \n",
    "        s = session\n",
    "        # if proxy:\n",
    "        #     requests.packages.urllib3.disable_warnings()\n",
    "        #     authenticator = '87783015bbe2d2f900e2f8be352c414a'\n",
    "        #     proxy_str = 'http://' + authenticator + '@' +'workdistribute.charityengine.com:20000'\n",
    "        #     s.proxies = {'http': proxy_str, 'https': proxy_str}\n",
    "        #     s.auth = HTTPProxyAuth(authenticator,'') \n",
    "\n",
    "        page = s.get(url, timeout=30, verify=False)\n",
    "        #page = requests.get(url)\n",
    "        tree = html.fromstring(page.content)\n",
    "       \n",
    "        map = tree.xpath('//div[@id=\"map\"]')\n",
    "\n",
    "        # Sometimes there's no location info, and no map on the page        \n",
    "        if len(map) == 0:\n",
    "            return [99, 99, 99]\n",
    "\n",
    "        map = map[0]\n",
    "        lat = map.xpath('@data-latitude')[0]\n",
    "        lng = map.xpath('@data-longitude')[0]\n",
    "        \n",
    "        \n",
    "        accuracy = map.xpath('@data-accuracy')[0]\n",
    "\n",
    "        return [lat, lng, accuracy]\n",
    "   \n",
    "    def PageAttributes(self, session, url, proxy=True):   \n",
    "        '''\n",
    "        Here we're parsing through the section in each listing that provides amenity information in one long string of text within a span tag \n",
    "        '''\n",
    "        \n",
    "        s = session\n",
    "         \n",
    "        page = s.get(url, timeout=30, verify=False)  \n",
    "        tree = html.fromstring(page.content)\n",
    "        \n",
    "        attrs  = tree.xpath('/html/body/section/section/section/div[1]/p[2]/span') \n",
    "\n",
    "        furnished = any(['furnished' in attr.text for attr in attrs]) # A \"False\" doesn't necessarily mean the unit isn't furnished\n",
    "    \n",
    "        laundry_possible1 = any(['laundry' in attr.text for attr in attrs])\n",
    "        laundry_possible2 = any(['w/d' in attr.text for attr in attrs])\n",
    "        \n",
    "        if (laundry_possible1 == True or laundry_possible2 == True):\n",
    "            laundry_known = 'TRUE'\n",
    "        else:\n",
    "            laundry_known = 'FALSE'\n",
    "            \n",
    "        no_laundryonsite1 = any(['no laundry' in attr.text for attr in attrs])\n",
    "        no_laundryonsite2 = any(['hookups' in attr.text for attr in attrs]) #haven't come accross one of these yet so not positive it's working\n",
    "        \n",
    "        if (no_laundryonsite1 == True or no_laundryonsite2 == True):\n",
    "            no_laundryonsite = 'TRUE'\n",
    "        else:\n",
    "            no_laundryonsite = 'FALSE' \n",
    "        \n",
    "        laundry_inunit = any(['w/d in unit' in attr.text for attr in attrs])   \n",
    "        \n",
    "        if (laundry_known == 'TRUE' and no_laundryonsite == 'FALSE' and laundry_inunit == False): \n",
    "            laundry_onpremises = 'TRUE'    \n",
    "        else:\n",
    "            laundry_onpremises = 'FALSE'      \n",
    "\n",
    "        room_known = any(['room' in attr.text for attr in attrs])\n",
    "        \n",
    "        private_room1 = any(['private room' in attr.text for attr in attrs])\n",
    "        \n",
    "        if(room_known == True and private_room1 == True):\n",
    "            private_room = 'TRUE'\n",
    "        else:\n",
    "            private_room = 'FALSE'\n",
    "                        \n",
    "        bath_known = any(['bath' in attr.text for attr in attrs])\n",
    "        bath_possible = any(['private bath' in attr.text for attr in attrs])\n",
    "        no_bath = any(['no private bath' in attr.text for attr in attrs])  \n",
    "        if(bath_possible == True and no_bath== False):           \n",
    "            private_bath = 'TRUE' \n",
    "        else:\n",
    "            private_bath = 'FALSE'\n",
    "        \n",
    "        parking_knowna = any(['carport' in attr.text for attr in attrs])\n",
    "        parking_knownb = any(['attached garage' in attr.text for attr in attrs])\n",
    "        parking_knownc = any(['off-street parking' in attr.text for attr in attrs])\n",
    "        parking_knownd = any(['detached garage' in attr.text for attr in attrs])\n",
    "        parking_knowne = any(['street parking' in attr.text for attr in attrs])\n",
    "        parking_knownf = any(['valet parking' in attr.text for attr in attrs])\n",
    "        parking_knowng = any(['no parking' in attr.text for attr in attrs])\n",
    "        if(parking_knowna == True or parking_knownb== True or parking_knownc== True or parking_knownd== True or parking_knowne== True or parking_knownf== True or parking_knowng== True):           \n",
    "            parking_known = 'TRUE' \n",
    "        else:\n",
    "            parking_known = 'FALSE'\n",
    "        \n",
    "        parking_poss1 = any(['carport' in attr.text for attr in attrs])\n",
    "        parking_poss2 = any(['attached garage' in attr.text for attr in attrs])\n",
    "        parking_poss3 = any(['off-street parking' in attr.text for attr in attrs])\n",
    "        parking_poss4 = any(['detached garage' in attr.text for attr in attrs])\n",
    "        parking_poss5 = any(['valet parking' in attr.text for attr in attrs])\n",
    "        if(parking_poss1 == True or parking_poss2== True or parking_poss3== True or parking_poss4== True or parking_poss5== True):           \n",
    "            parking_poss = 'TRUE' \n",
    "        else:\n",
    "            parking_poss = 'FALSE'                     \n",
    "                             \n",
    "        no_onsiteparking1 = any(['no parking' in attr.text for attr in attrs])\n",
    "        no_onsiteparking2 = any(['street parking' in attr.text for attr in attrs])\n",
    "        if(no_onsiteparking1 == True or no_onsiteparking2== True):           \n",
    "            no_onsiteparking = 'TRUE' \n",
    "        else:\n",
    "            no_onsiteparking = 'FALSE'\n",
    "         \n",
    "        if (parking_poss == 'TRUE' and no_onsiteparking == 'FALSE'):\n",
    "             parking_onsite = 'TRUE'\n",
    "        else:\n",
    "             parking_onsite = 'FALSE'\n",
    "\n",
    "        return [furnished, laundry_known, laundry_onpremises, laundry_inunit, room_known, private_room, bath_known, private_bath, parking_known, parking_onsite]    \n",
    "     \n",
    "    def _get_fips(self, row):\n",
    "        url = 'http://data.fcc.gov/api/block/find?format=json&latitude={}&longitude={}'\n",
    "        request = url.format(row['latitude'], row['longitude'])\n",
    "        # TO DO: exception handling\n",
    "        response = requests.get(request)\n",
    "        data = response.json()\n",
    "        return pd.Series({'fips_block':data['Block']['FIPS'], 'state':data['State']['code'], 'county':data['County']['name']})\n",
    "\n",
    "    def _clean_listings(self, filename):\n",
    "\n",
    "            converters = {'neighb':str, \n",
    "                  'title':str, \n",
    "                  'price':self._toFloat, \n",
    "                  'pid':str, \n",
    "                  'dt':str, \n",
    "                  'url':str, \n",
    "                  'sqft':self._toFloat, \n",
    "                  'lng':self._toFloat, \n",
    "                  'lat':self._toFloat}\n",
    "\n",
    "            all_listings = pd.read_csv(filename, converters=converters)\n",
    "\n",
    "            if len(all_listings) == 0:\n",
    "                return [], 0, 0, 0\n",
    "                print('{0} total listings'.format(len(all_listings)))\n",
    "\n",
    "            all_listings = all_listings.rename(columns={'price':'rent', 'dt':'date', 'neighb':'neighborhood',\n",
    "                                                        'lng':'longitude', 'lat':'latitude'})\n",
    "            all_listings['rent_sqft'] = all_listings['rent'] / all_listings['sqft']\n",
    "            all_listings['date'] = pd.to_datetime(all_listings['date'], format='%Y-%m-%d')\n",
    "            all_listings['day_of_week'] = all_listings['date'].apply(lambda x: x.weekday())\n",
    "            all_listings['region'] = all_listings['url'].str.extract('http://(.*).craigslist.org', expand=False)\n",
    "            unique_listings = pd.DataFrame(all_listings.drop_duplicates(subset='pid', inplace=False))\n",
    "            thorough_listings = pd.DataFrame(unique_listings)\n",
    "            if len(thorough_listings) == 0:\n",
    "                return [], 0, 0, 0\n",
    "\n",
    "            cols = ['pid', 'date', 'day_of_week', 'url', 'title', 'rent', 'rent_sqft', 'neighborhood', 'region', 'sqft', 'latitude', 'longitude',\n",
    "                    'accuracy', 'body_text', 'furnished', 'laundry_known', 'laundry_onpremises', \n",
    "                    'laundry_inunit','room_known', 'private_room', 'bath_known', 'private_bath' 'parking_known', \n",
    "                    'onsite_parking']\n",
    "            data_output = thorough_listings[cols]\n",
    "            #we are not including fips codes yet: still need to resolve missing lat-long issue (won't work with fips code function)\n",
    "            #fips = data_output.apply(self._get_fips, axis=1)             \n",
    "            #geocoded = pd.concat([data_output, fips], axis=1)\n",
    "            return data_output, len(all_listings), len(thorough_listings), len(data_output)\n",
    "    \n",
    "    def _write_db(self, dataframe, domain):\n",
    "        '''\n",
    "        This function takes in the cleaned dataframe from the cleaning function\n",
    "        and exports it to a PostgreSQL database table.\n",
    "        '''\n",
    "        dbname = settings['dbname']\n",
    "        user = settings['user']\n",
    "        host = settings['host']\n",
    "        passwd = settings['password']\n",
    "        conn_str = \"dbname={0} user={1} host={2} password={3}\".format(dbname,user,host,passwd)\n",
    "        conn = psycopg2.connect(conn_str)\n",
    "        cur = conn.cursor()\n",
    "        num_listings = len(dataframe)\n",
    "        # print(\"Inserting {0} listings from {1} into database.\".format(num_listings, domain))\n",
    "        prob_PIDs = []\n",
    "        dupes = []\n",
    "        writes = []\n",
    "        for i,row in dataframe.iterrows():\n",
    "            try:\n",
    "                cur.execute('''INSERT INTO shared_listings\n",
    "                    VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s, %s, %s, %s)''',\n",
    "                    (row['pid'],pd.to_datetime(row['date']), row['day_of_week'], row['url'],row['title'],\n",
    "                    row['rent'], row['rent_sqft'], row['neighborhood'], row['region'], row['sqft'],row['latitude'],\n",
    "                    row['longitude'],row['accuracy'],row['body_text'],\n",
    "                    row['furnished'],row['laundry_known'],row['laundry_onpremises'],\n",
    "                    row['laundry_inunit'],row['room_known'],row['private_room'],\n",
    "                    row['bath_known'],row['private_bath'],row['parking_known'],row['onsite_parking']))\n",
    "                conn.commit()\n",
    "                writes.append(row['pid'])\n",
    "\n",
    "            except Exception as e:\n",
    "                if 'duplicate key value violates unique' in str(e):\n",
    "                    dupes.append(row['pid'])\n",
    "                else:\n",
    "                    prob_PIDs.append(str(row['pid']))\n",
    "                conn.rollback()\n",
    "\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "        return prob_PIDs, dupes, writes\n",
    "    \n",
    "    def run(self, charity_proxy=True):\n",
    "        \n",
    "            colnames = ['pid','dt','url','title','price','neighb','sqft',\n",
    "                        'lat','lng','accuracy','body_text', 'furnished', 'laundry_known', 'laundry_onpremises', 'laundry_inunit', 'room_known', 'private_room', 'bath_known', 'private_bath', 'parking_known', 'onsite_parking']     \n",
    "            st_time = time.time()\n",
    "        \n",
    "            #st+time = time.time()\n",
    "            #LOOP ALL REGIONS ONE DOMAIN AT A TIME\n",
    "            for domain in self.domains:\n",
    "                \n",
    "                total_listings = 0\n",
    "                listing_num = 0\n",
    "                ts_skipped = 0\n",
    "                \n",
    "                regionName = domain.split('//')[1].split('.craigslist')[0]\n",
    "                fname = self.out_dir + self.fname_base + '-' + regionName + (self.ts if self.fname_ts else '') + '.csv'\n",
    "                regionIsComplete = False\n",
    "                search_url = domain\n",
    "                print(\"beginning new region\")\n",
    "                logging.info('BEGINNING NEW REGION')\n",
    "                        \n",
    "                with open(fname, 'wb') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(colnames)\n",
    "                    \n",
    "                    while not regionIsComplete:\n",
    "                        \n",
    "                        logging.info(search_url)\n",
    "                        s = requests.Session()\n",
    "                        \n",
    "                        if charity_proxy:\n",
    "                            requests.packages.urllib3.disable_warnings()\n",
    "                            authenticator = '87783015bbe2d2f900e2f8be352c414a'\n",
    "                            proxy_str = 'http://' + authenticator +':foo'+ '@' +'workdistribute.charityengine.com:20000'\n",
    "                            s.proxies = {'http': proxy_str, 'https': proxy_str}\n",
    "                            s.auth = HTTPProxyAuth(authenticator,'')\n",
    "\n",
    "                        try:\n",
    "                            page = s.get(search_url, timeout=30, verify=False)\n",
    "                        except requests.exceptions.Timeout:\n",
    "                            s = requests.Session()\n",
    "                            if charity_proxy:\n",
    "                                s.proxies = {'http': proxy_str, 'https': proxy_str}\n",
    "                                s.auth = HTTPProxyAuth(authenticator,'')\n",
    "                            try:\n",
    "                                page = s.get(search_url, timeout=30, verify=False)    \n",
    "                            except:\n",
    "                                regionIsComplete = True\n",
    "                                logging.info('FAILED TO CONNECT.')\n",
    "\n",
    "                        try:\n",
    "                            tree = html.fromstring(page.content)\n",
    "                        except:\n",
    "                            regionIsComplete = True\n",
    "                            logging.info('FAILED TO PARSE HTML.')\n",
    "                        \n",
    "                        \n",
    "                        #page = requests.get(search_url)\n",
    "                        print(page.status_code)\n",
    "                        tree = html.fromstring(page.content)\n",
    "                        #return tree\n",
    "                            \n",
    "                        listings = tree.xpath('//li[@class=\"result-row\"]')\n",
    "                        print(\"got {0} listings\".format(len(listings)))\n",
    "                        \n",
    "                        if len(listings) == 0 and total_listings == 0:\n",
    "                            logging.info('NO LISTINGS RETRIEVED FOR {0}'.format(str.upper(regionName)))\n",
    "\n",
    "                        total_listings += len(listings)\n",
    "                        \n",
    "                        for item in listings:\n",
    "                            listing_num += 1\n",
    "                            try:\n",
    "                                row = self._parseListing(item)\n",
    "                                item_ts = dt.strptime(row[1], '%Y-%m-%d %H:%M')\n",
    "                \n",
    "                                if (item_ts > self.latest_ts):\n",
    "                                # Skip this item but continue parsing search results\n",
    "                                    ts_skipped += 1\n",
    "                                    continue\n",
    "\n",
    "                                if (item_ts < self.earliest_ts):\n",
    "                                # Break out of loop and move on to the next region\n",
    "                                    if listing_num == 1:\n",
    "                                        logging.info('NO LISTINGS BEFORE TIMESTAMP CUTOFF AT {0}'.format(str.upper(regionName)))    \n",
    "                                    else:\n",
    "                                        logging.info('REACHED TIMESTAMP CUTOFF')\n",
    "                                    ts_skipped += 1\n",
    "                                    regionIsComplete = True\n",
    "                                    logging.info('REACHED TIMESTAMP CUTOFF')\n",
    "                                    break \n",
    "                    \n",
    "                                item_url = domain.split('/search/roo')[0] + row[2]\n",
    "                                row[2] = item_url\n",
    "                                #item_url = domain.split('/search')[0] + tree.xpath('a/@href')[0]\n",
    "                                logging.info(item_url)\n",
    "                                row += self._scrapeLatLng(s, item_url)\n",
    "                                row += self.PageBodyText(s, item_url)\n",
    "                                row += self.PageAttributes(s, item_url)\n",
    "                                writer.writerow(row)\n",
    "                            \n",
    "                            except Exception as e:\n",
    "                            # Skip listing if there are problems parsing it\n",
    "                                logging.warning(\"{0}: {1}. Probably no beds/sqft info\".format(type(e).__name__, e))\n",
    "                                continue\n",
    "                                   \n",
    "                        next = tree.xpath('//a[@title=\"next page\"]/@href')\n",
    "                        if len(next) > 0:\n",
    "                            search_url = domain.split('/search')[0] + next[0]\n",
    "                        else:\n",
    "                            regionIsComplete = True\n",
    "                            logging.info('RECEIVED ERROR PAGE')       \n",
    "                        s.close()\n",
    "                #print tr_skipped\n",
    "                if ts_skipped == total_listings:\n",
    "                    logging.info(('{0} TIMESTAMPS NOT MATCHING' + '- CL: {1} vs. ual: {2}.' + ' NO DATA SAVED.').format(regionName,str(item_ts),str(self.latest_ts)))\n",
    "                    continue\n",
    "                #passing for now. When finished writing cleaning function, we will expand on this section\n",
    "                cleaned, count_listings, count_thorough, count_geocoded = self._clean_listings(fname)\n",
    "                num_cleaned = len(cleaned)\n",
    "                #cleaned.to_csv(OUT_DIR+\"/cleaned.csv\") this step doesn't seem to be working, but shouldn't be necessary.\n",
    "                if num_cleaned >0:\n",
    "                    probs, dupes, writes = self._write_db(cleaned, domain)\n",
    "                    num_probs = len(probs)\n",
    "                    num_dupes = len(dupes)\n",
    "                    num_writes = len(writes)\n",
    "                    pct_written = (num_writes) / num_cleaned * 100\n",
    "                    pct_fail = round(num_probs / num_cleaned * 100,3)\n",
    "                    if num_dupes == num_cleaned:\n",
    "                        logging.info('100% OF {0} PIDS ARE DUPES. NOTHING WRITTEN'.format(str.upper(regionName)))\n",
    "                    \n",
    "                    else:\n",
    "                        logging.info('FAILED TO WRITE {0}% OF {1} PIDS:'.format(pct_fail,str.upper(regionName)) + ', '.join(probs))\n",
    "                else: \n",
    "                    #passing for now. When finished writing cleaning function, we will expand on this section\n",
    "                    pass\n",
    "                \n",
    "            return   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scraper = RentalListingScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning new region\n",
      "200\n",
      "got 120 listings\n",
      "200\n",
      "got 120 listings\n",
      "200\n",
      "got 120 listings\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m                 \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mtimeout\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    792\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mtimeout\u001b[0m: The read operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-2025965eb624>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscraper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-664a949b6381>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, charity_proxy)\u001b[0m\n\u001b[0;32m    435\u001b[0m                                 \u001b[1;31m#item_url = domain.split('/search')[0] + tree.xpath('a/@href')[0]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m                                 \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m                                 \u001b[0mrow\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_scrapeLatLng\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m                                 \u001b[0mrow\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPageBodyText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m                                 \u001b[0mrow\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPageAttributes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mitem_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-664a949b6381>\u001b[0m in \u001b[0;36m_scrapeLatLng\u001b[1;34m(self, session, url, proxy)\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m#     s.auth = HTTPProxyAuth(authenticator,'')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[1;31m#page = requests.get(url)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0mtree\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfromstring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GET'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    473\u001b[0m         }\n\u001b[0;32m    474\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 475\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    476\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    477\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m         \u001b[1;31m# Resolve redirects if allowed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m         \u001b[1;31m# Shuffle things around if there's history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m         \u001b[1;31m# Resolve redirects if allowed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m         \u001b[1;31m# Shuffle things around if there's history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    175\u001b[0m                 \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 177\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    178\u001b[0m             )\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 596\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    421\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m                 )\n\u001b[0;32m    425\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, **response_kw)\u001b[0m\n\u001b[0;32m    593\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    594\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m                                                   chunked=chunked)\n\u001b[0m\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m             \u001b[1;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[1;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[0;32m    387\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Python 2.6 and older, Python 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m                     \u001b[0mhttplib_response\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m                     \u001b[1;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1197\u001b[1;33m                 \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1198\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mbegin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    295\u001b[0m         \u001b[1;31m# read until we get a non-100 response\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 297\u001b[1;33m             \u001b[0mversion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    298\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iso-8859-1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"status line\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    573\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m    927\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m                   self.__class__)\n\u001b[1;32m--> 929\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    930\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\james\\Anaconda3\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    792\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "scraper.run()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
