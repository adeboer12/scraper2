{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "import importlib\n",
    "import urllib\n",
    "import unicodecsv as csv\n",
    "from lxml import html\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "from requests.auth import HTTPProxyAuth\n",
    "import time\n",
    "#import syslog\n",
    "#import psycopg2\n",
    "#import shutil\n",
    "#import os\n",
    "#import glob\n",
    "#import subprocess\n",
    "#from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#charity, proxy, s, sessions variables are associated with the charity engine\n",
    "#Try to run this with San Francisco\n",
    "DOMAINS = ['http://sfbay.craigslist.org/search/roo','http://modesto.craigslist.org/search/roo'] \n",
    "\n",
    "#Craigslist doesn't use time zones in its timestamps, so these cutoffs will be\n",
    "#interpreted relative to the local time at the listing location. For example, dt.now()\n",
    "#run from a machine in San Francisco will match listings from 3 hours ago in Boston.\n",
    "LATEST_TS = dt.now()\n",
    "EARLIEST_TS = LATEST_TS - timedelta(hours=1)\n",
    "\n",
    "\n",
    "#OUT_DIR =\"C:\\\\Users\\\\james\\\\Documents\\\\Berkeley_Docs\\\\Spring_17_Courses\\\\CP290 Data Lab\\\\scraper_output\\\\\" #James's directory\n",
    "OUT_DIR =\"C:\\\\Users\\\\varun\\\\Documents\\\\Berkeley\\\\2017 Spring\\\\Workshop\\\\Datasets\\\\data_output\\\\\" #Varun's directory\n",
    "#OUT_DIR ='/Users/anniedbr/Desktop/CSV/'  #Annie's directory\n",
    "\n",
    "FNAME_BASE = 'data'  # filename prefix for saved data\n",
    "FNAME_TS = True  # append timestamp to filename\n",
    "\n",
    "S3_UPLOAD = False\n",
    "S3_BUCKET = 'scraper2'\n",
    "\n",
    "class RentalListingScraper(object):\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            domains = DOMAINS,\n",
    "            earliest_ts = EARLIEST_TS,\n",
    "            latest_ts = LATEST_TS, \n",
    "            out_dir = OUT_DIR,\n",
    "            fname_base = FNAME_BASE,\n",
    "            fname_ts = FNAME_TS,\n",
    "            s3_upload = S3_UPLOAD,\n",
    "            s3_bucket = S3_BUCKET):\n",
    "        \n",
    "        self.domains = domains\n",
    "        self.earliest_ts = earliest_ts\n",
    "        self.latest_ts = latest_ts\n",
    "        self.out_dir = out_dir\n",
    "        self.fname_base = fname_base\n",
    "        self.fname_ts = fname_ts\n",
    "        self.s3_upload = s3_upload\n",
    "        self.s3_bucket = s3_bucket\n",
    "        self.ts = dt.now().strftime('%Y%m%d-%H%M%S')  # Use timestamp as file id\n",
    "        #self.ts = fname_ts\n",
    "\n",
    "        log_fname = self.out_dir + self.fname_base \\\n",
    "                + (self.ts if self.fname_ts else '') + '.log'\n",
    "        \n",
    "        importlib.reload(logging)\n",
    "        \n",
    "        logging.basicConfig(filename=log_fname, level=logging.INFO)\n",
    "       \n",
    "        #Suppress info messages from the 'requests' library\n",
    "        #logging.getLogger('requests').setLevel(logging.WARNING)  \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "    def _get_str(self, list):\n",
    "        '''\n",
    "        The xpath() function returns a list of items that may be empty. Most of the time,\n",
    "        we want the first of any strings that match the xml query. This helper function\n",
    "        returns that string, or null if the list is empty.\n",
    "        '''\n",
    "        \n",
    "        if len(list) > 0:\n",
    "            return list[0]\n",
    "\n",
    "        return ''\n",
    "    \n",
    "        \n",
    "    def _get_int_prefix(self, str, label):\n",
    "        '''\n",
    "        Bedrooms and square footage have the format \"xx 1br xx 450ft xx\". This helper \n",
    "        function extracts relevant integers from strings of this format.\n",
    "        '''     \n",
    "        \n",
    "        for s in str.split(' '):\n",
    "            if label in s:\n",
    "                return s.strip(label)\n",
    "                \n",
    "        return 0\n",
    "\n",
    "\n",
    "    def _toFloat(self, string_value):\n",
    "        string_value = string_value.strip()\n",
    "        return np.float(string_value) if string_value else np.nan\n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "    def _parseListing(self, item):\n",
    "        '''\n",
    "        Note that xpath() returns a list with elements of varying types depending on the\n",
    "        query results: xml objects, strings, etc.\n",
    "        '''\n",
    "        pid = item.xpath('@data-pid')[0]  # post id, always present\n",
    "        info = item.xpath('p[@class=\"result-info\"]')[0]\n",
    "        dt = info.xpath('time/@datetime')[0]\n",
    "        url = info.xpath('a/@href')[0]\n",
    "        if type(info.xpath('a/text()')) == str:\n",
    "            title = info.xpath('a/text()')\n",
    "        else:\n",
    "            title = info.xpath('a/text()')[0]\n",
    "        price = self._get_str(info.xpath('span[@class=\"result-meta\"]/span[@class=\"result-price\"]/text()')).strip('$')\n",
    "        neighb_raw = info.xpath('span[@class=\"result-meta\"]/span[@class=\"result-hood\"]/text()')\n",
    "        if len(neighb_raw) == 0:\n",
    "            neighb = ''\n",
    "        else:\n",
    "            neighb = neighb_raw[0].strip(\" \").strip(\"(\").strip(\")\")\n",
    "        housing_raw = info.xpath('span[@class=\"result-meta\"]/span[@class=\"housing\"]/text()')\n",
    "        if len(housing_raw) == 0:\n",
    "            #beds = 0\n",
    "            sqft = 0\n",
    "        else:\n",
    "            bedsqft = housing_raw[0]\n",
    "            #beds = self._get_int_prefix(bedsqft, \"br\")  # appears as \"1br\" to \"8br\" or missing\n",
    "            sqft = self._get_int_prefix(bedsqft, \"ft\")  # appears as \"000ft\" or missing\n",
    "        #for domain in self.domains:\n",
    "            #url = domain.split('/search')[0] + info.xpath('a/@href')[0]\n",
    "        #return [pid, dt, url, title, price, neighb, beds, sqft]\n",
    "        return [pid, dt, url, title, price, neighb, sqft]\n",
    "\n",
    "    \n",
    "# ctrl + / for making selection into comments\n",
    "#     def _parseAddress(self, tree):\n",
    "#         '''\n",
    "#         Some listings include an address, but we have to parse it out of an encoded\n",
    "#         Google Maps url.\n",
    "#         '''\n",
    "#         url = self._get_str(tree.xpath('//p[@class=\"mapaddress\"]/small/a/@href'))\n",
    "        \n",
    "#         if '?q=loc' not in url:\n",
    "#             # That string precedes an address search\n",
    "#             return ''\n",
    "            \n",
    "#         return urllib.unquote_plus(url.split('?q=loc')[1]).strip(' :')\n",
    "    \n",
    "#     #def PageBodyText(self, session, url, proxy=True):\n",
    "#     #We've tried section, div\n",
    "    \n",
    "    def PageBodyText(self, session, url, proxy=True):\n",
    "        #this grabs the entire XML structured text from each post, then cleans it a bit.  \n",
    "        \n",
    "        s = session\n",
    "        #page = requests.get(url)        \n",
    "        page = s.get(url, timeout=30)\n",
    "        tree = html.fromstring(page.content)\n",
    "        path = tree.xpath('//section[@id=\"postingbody\"]')[0]\n",
    "               \n",
    "        body_list = path.xpath('text()')\n",
    "        \n",
    "        body_text = ''.join(body_list).strip().encode('utf-8')\n",
    "        \n",
    "        return [body_text]\n",
    "     \n",
    "    def _scrapeLatLng(self, session, url, proxy=True):\n",
    "    \n",
    "        s = session\n",
    "        # if proxy:\n",
    "        #     requests.packages.urllib3.disable_warnings()\n",
    "        #     authenticator = '87783015bbe2d2f900e2f8be352c414a'\n",
    "        #     proxy_str = 'http://' + authenticator + '@' +'workdistribute.charityengine.com:20000'\n",
    "        #     s.proxies = {'http': proxy_str, 'https': proxy_str}\n",
    "        #     s.auth = HTTPProxyAuth(authenticator,'') \n",
    "\n",
    "        page = s.get(url, timeout=30)\n",
    "        #page = requests.get(url)\n",
    "        tree = html.fromstring(page.content)\n",
    "       \n",
    "        map = tree.xpath('//div[@id=\"map\"]')\n",
    "\n",
    "        # Sometimes there's no location info, and no map on the page        \n",
    "        if len(map) == 0:\n",
    "            return ['', '','']\n",
    "\n",
    "        map = map[0]\n",
    "        lat = map.xpath('@data-latitude')[0]\n",
    "        lng = map.xpath('@data-longitude')[0]\n",
    "        \n",
    "        \n",
    "        accuracy = map.xpath('@data-accuracy')[0]\n",
    "\n",
    "        #address = self._parseAddress(tree)\n",
    "        \n",
    "        return [lat, lng, accuracy]\n",
    "   \n",
    "    def PageAttributes(self, session, url, proxy=True):   \n",
    "        '''\n",
    "        Here we're parsing through the section in each listing that provides amenity information in one long string of text within a span tag \n",
    "        '''\n",
    "        \n",
    "        s = session\n",
    "         \n",
    "        page = s.get(url, timeout=30)        \n",
    "        #page = requests.get(url)\n",
    "        tree = html.fromstring(page.content)\n",
    "        \n",
    "        attrs  = tree.xpath('/html/body/section/section/section/div[1]/p[2]/span') \n",
    "        \n",
    "#         laundry_known = any((['laundry on site' in attr.text for attr in attrs]) | (['w/d in unit' in attr.text for attr in attrs]))\n",
    "#         laundry_possible = any((['laundry on site' in attr.text for attr in attrs]) or (['laundry in bldg' in attr.text for attr in attrs])) \n",
    "        \n",
    "        furnished = any(['furnished' in attr.text for attr in attrs]) # A \"False\" doesn't necessarily mean the unit isn't furnished\n",
    "    \n",
    "        laundry_possible1 = any(['laundry' in attr.text for attr in attrs])\n",
    "        laundry_possible2 = any(['w/d' in attr.text for attr in attrs])\n",
    "        \n",
    "        if (laundry_possible1 == True or laundry_possible2 == True):\n",
    "            laundry_known = 'TRUE'\n",
    "        else:\n",
    "            laundry_known = 'FALSE'\n",
    "            \n",
    "        no_laundryonsite1 = any(['no laundry' in attr.text for attr in attrs])\n",
    "        no_laundryonsite2 = any(['hookups' in attr.text for attr in attrs]) #haven't come accross one of these yet so not positive it's working\n",
    "        \n",
    "        if (no_laundryonsite1 == True or no_laundryonsite2 == True):\n",
    "            no_laundryonsite = 'TRUE'\n",
    "        else:\n",
    "            no_laundryonsite = 'FALSE' \n",
    "        \n",
    "        laundry_inunit = any(['w/d in unit' in attr.text for attr in attrs])   \n",
    "        \n",
    "        if (laundry_known == 'TRUE' and no_laundryonsite == 'FALSE' and laundry_inunit == False): \n",
    "            laundry_onpremises = 'TRUE'    \n",
    "        else:\n",
    "            laundry_onpremises = 'FALSE'      \n",
    "\n",
    "        room_known = any(['room' in attr.text for attr in attrs])\n",
    "        \n",
    "        private_room1 = any(['private room' in attr.text for attr in attrs])\n",
    "        \n",
    "        if(room_known == True and private_room1 == True):\n",
    "            private_room = 'TRUE'\n",
    "        else:\n",
    "            private_room = 'FALSE'\n",
    "                        \n",
    "        bath_known = any(['bath' in attr.text for attr in attrs])\n",
    "        bath_possible = any(['private bath' in attr.text for attr in attrs])\n",
    "        no_bath = any(['no private bath' in attr.text for attr in attrs])  \n",
    "        if(bath_possible == True and no_bath== False):           \n",
    "            private_bath = 'TRUE' \n",
    "        else:\n",
    "            private_bath = 'FALSE'\n",
    "        \n",
    "        parking_knowna = any(['carport' in attr.text for attr in attrs])\n",
    "        parking_knownb = any(['attached garage' in attr.text for attr in attrs])\n",
    "        parking_knownc = any(['off-street parking' in attr.text for attr in attrs])\n",
    "        parking_knownd = any(['detached garage' in attr.text for attr in attrs])\n",
    "        parking_knowne = any(['street parking' in attr.text for attr in attrs])\n",
    "        parking_knownf = any(['valet parking' in attr.text for attr in attrs])\n",
    "        parking_knowng = any(['no parking' in attr.text for attr in attrs])\n",
    "        if(parking_knowna == True or parking_knownb== True or parking_knownc== True or parking_knownd== True or parking_knowne== True or parking_knownf== True or parking_knowng== True):           \n",
    "            parking_known = 'TRUE' \n",
    "        else:\n",
    "            parking_known = 'FALSE'\n",
    "        \n",
    "        parking_poss1 = any(['carport' in attr.text for attr in attrs])\n",
    "        parking_poss2 = any(['attached garage' in attr.text for attr in attrs])\n",
    "        parking_poss3 = any(['off-street parking' in attr.text for attr in attrs])\n",
    "        parking_poss4 = any(['detached garage' in attr.text for attr in attrs])\n",
    "        parking_poss5 = any(['valet parking' in attr.text for attr in attrs])\n",
    "        if(parking_poss1 == True or parking_poss2== True or parking_poss3== True or parking_poss4== True or parking_poss5== True):           \n",
    "            parking_poss = 'TRUE' \n",
    "        else:\n",
    "            parking_poss = 'FALSE'                     \n",
    "                             \n",
    "        no_onsiteparking1 = any(['no parking' in attr.text for attr in attrs])\n",
    "        no_onsiteparking2 = any(['street parking' in attr.text for attr in attrs])\n",
    "        if(no_onsiteparking1 == True or no_onsiteparking2== True):           \n",
    "            no_onsiteparking = 'TRUE' \n",
    "        else:\n",
    "            no_onsiteparking = 'FALSE'\n",
    "         \n",
    "        if (parking_poss == 'TRUE' and no_onsiteparking == 'FALSE'):\n",
    "             parking_onsite = 'TRUE'\n",
    "        else:\n",
    "             parking_onsite = 'FALSE'\n",
    "        \n",
    "#Old Attempt at the parking stuff\n",
    "#         parking_known = any((['carport' in attr.text for attr in attrs]) or (['attached garage' in attr.text for attr in attrs]) or (['off-street parking' in attr.text for attr in attrs]) or (['detached garage' in attr.text for attr in attrs]) or (['street parking' in attr.text for attr in attrs]) or (['valet parking' in attr.text for attr in attrs]) or (['no parking' in attr.text for attr in attrs]))   \n",
    "#         parking_possible = any((['carport' in attr.text for attr in attrs]) or (['attached garage' in attr.text for attr in attrs]) or (['off-street parking' in attr.text for attr in attrs]) or (['valet parking' in attr.text for attr in attrs]) or (['detached garage' in attr.text for attr in attrs]))\n",
    "#         no_onsiteparking = any((['no parking' in attr.text for attr in attrs]) or (['street parking' in attr.text for attr in attrs]))\n",
    "#         #onsite parking is going to include the following categories: carport, attached garage, detached garage, off-street, valet parking\n",
    "#         if (parking_possible == True and no_onsiteparking == False):\n",
    "#             parking_onsite = 'TRUE'\n",
    "#         else:\n",
    "#             parking_onsite = 'FALSE'\n",
    "\n",
    "        return [furnished, laundry_known, laundry_onpremises, laundry_inunit, room_known, private_room, bath_known, private_bath, parking_known, parking_onsite]    \n",
    "     \n",
    "          \n",
    "    def run(self, charity_proxy=True):\n",
    "        \n",
    "            colnames = ['pid','dt','url','title','price','neighb','sqft',\n",
    "                        'lat','lng','accuracy','body_text', 'furnished', 'laundry_known', 'laundry_onpremises', 'laundry_inunit', 'room_known', 'private_room', 'bath_known', 'private_bath', 'parking_known', 'onsite_parking']     \n",
    "            st_time = time.time()\n",
    "        \n",
    "            #st+time = time.time()\n",
    "            #LOOP ALL REGIONS ONE DOMAIN AT A TIME\n",
    "            for domain in self.domains:\n",
    "                \n",
    "                total_listings = 0\n",
    "                listing_num = 0\n",
    "                ts_skipped = 0\n",
    "                \n",
    "                regionName = domain.split('//')[1].split('.craigslist')[0]\n",
    "                fname = self.out_dir + self.fname_base + '-' + regionName + (self.ts if self.fname_ts else '') + '.csv'\n",
    "                regionIsComplete = False\n",
    "                search_url = domain\n",
    "                print(\"beginning new region\")\n",
    "                logging.info('BEGINNING NEW REGION')\n",
    "                        \n",
    "                with open(fname, 'wb') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow(colnames)\n",
    "                    \n",
    "                    while not regionIsComplete:\n",
    "                        \n",
    "                        logging.info(search_url)\n",
    "                        s = requests.Session()\n",
    "                        \n",
    "                        if charity_proxy:\n",
    "                            requests.packages.urllib3.disable_warnings()\n",
    "                            authenticator = '87783015bbe2d2f900e2f8be352c414a'\n",
    "                            proxy_str = 'http://' + authenticator + '@' +'workdistribute.charityengine.com:20000'\n",
    "                            s.proxies = {'http': proxy_str, 'https': proxy_str}\n",
    "                            s.auth = HTTPProxyAuth(authenticator,'')\n",
    "\n",
    "                        try:\n",
    "                            page = s.get(search_url, timeout=30)\n",
    "                        except requests.exceptions.Timeout:\n",
    "                            s = requests.Session()\n",
    "                            if charity_proxy:\n",
    "                                s.proxies = {'http': proxy_str, 'https': proxy_str}\n",
    "                                s.auth = HTTPProxyAuth(authenticator,'')\n",
    "                            try:\n",
    "                                page = s.get(search_url, timeout=30)    \n",
    "                            except:\n",
    "                                regionIsComplete = True\n",
    "                                logging.info('FAILED TO CONNECT.')\n",
    "\n",
    "                        try:\n",
    "                            tree = html.fromstring(page.content)\n",
    "                        except:\n",
    "                            regionIsComplete = True\n",
    "                            logging.info('FAILED TO PARSE HTML.')\n",
    "                        \n",
    "                        \n",
    "                        #page = requests.get(search_url)\n",
    "                        print(page.status_code)\n",
    "                        tree = html.fromstring(page.content)\n",
    "                        #return tree\n",
    "                            \n",
    "                        listings = tree.xpath('//li[@class=\"result-row\"]')\n",
    "                        print(\"got {0} listings\".format(len(listings)))\n",
    "                        \n",
    "                        if len(listings) == 0 and total_listings == 0:\n",
    "                            logging.info('NO LISTINGS RETRIEVED FOR {0}'.format(str.upper(regionName)))\n",
    "\n",
    "                        total_listings += len(listings)\n",
    "                        \n",
    "                        for item in listings:\n",
    "                            listing_num += 1\n",
    "                            try:\n",
    "                                row = self._parseListing(item)\n",
    "                                item_ts = dt.strptime(row[1], '%Y-%m-%d %H:%M')\n",
    "                \n",
    "                                if (item_ts > self.latest_ts):\n",
    "                                # Skip this item but continue parsing search results\n",
    "                                    ts_skipped += 1\n",
    "                                    continue\n",
    "\n",
    "                                if (item_ts < self.earliest_ts):\n",
    "                                # Break out of loop and move on to the next region\n",
    "                                    if listing_num == 1:\n",
    "                                        logging.info('NO LISTINGS BEFORE TIMESTAMP CUTOFF AT {0}'.format(str.upper(regionName)))    \n",
    "                                    else:\n",
    "                                        logging.info('REACHED TIMESTAMP CUTOFF')\n",
    "                                    ts_skipped += 1\n",
    "                                    regionIsComplete = True\n",
    "                                    logging.info('REACHED TIMESTAMP CUTOFF')\n",
    "                                    break \n",
    "                    \n",
    "                                item_url = domain.split('/search/roo')[0] + row[2]\n",
    "                                row[2] = item_url\n",
    "                                #item_url = domain.split('/search')[0] + tree.xpath('a/@href')[0]\n",
    "                                logging.info(item_url)\n",
    "                                row += self._scrapeLatLng(s, item_url)\n",
    "                                row += self.PageBodyText(s, item_url)\n",
    "                                row += self.PageAttributes(s, item_url)\n",
    "                                writer.writerow(row)\n",
    "                            \n",
    "                            except Exception as e:\n",
    "                            # Skip listing if there are problems parsing it\n",
    "                                logging.warning(\"{0}: {1}. Probably no beds/sqft info\".format(type(e).__name__, e))\n",
    "                                continue\n",
    "                                   \n",
    "                        next = tree.xpath('//a[@title=\"next page\"]/@href')\n",
    "                        if len(next) > 0:\n",
    "                            search_url = domain.split('/search')[0] + next[0]\n",
    "                        else:\n",
    "                            regionIsComplete = True\n",
    "                            logging.info('RECEIVED ERROR PAGE')       \n",
    "                        s.close()\n",
    "                #print tr_skipped\n",
    "                if ts_skipped == total_listings:\n",
    "                    logging.info(('{0} TIMESTAMPS NOT MATCHING' + '- CL: {1} vs. ual: {2}.' + ' NO DATA SAVED.').format(regionName,str(item_ts),str(self.latest_ts)))\n",
    "                    continue\n",
    "            return   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scraper = RentalListingScraper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beginning new region\n"
     ]
    },
    {
     "ename": "ProxyError",
     "evalue": "HTTPSConnectionPool(host='sfbay.craigslist.org', port=443): Max retries exceeded with url: /search/roo (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy authentication required',)))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    593\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_new_proxy_conn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_proxy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36m_prepare_proxy\u001b[1;34m(self, conn)\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connection.py\u001b[0m in \u001b[0;36mconnect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    293\u001b[0m             \u001b[1;31m# self._tunnel_host below.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tunnel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[1;31m# Mark this connection as not reusable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_tunnel\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             raise OSError(\"Tunnel connection failed: %d %s\" % (code,\n\u001b[1;32m--> 832\u001b[1;33m                                                                message.strip()))\n\u001b[0m\u001b[0;32m    833\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Tunnel connection failed: 407 Proxy authentication required",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    422\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m                     \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m                 )\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[0;32m    648\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[1;32m--> 649\u001b[1;33m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[0;32m    650\u001b[0m             \u001b[0mretries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\requests\\packages\\urllib3\\util\\retry.py\u001b[0m in \u001b[0;36mincrement\u001b[1;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merror\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='sfbay.craigslist.org', port=443): Max retries exceeded with url: /search/roo (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy authentication required',)))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mProxyError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2025965eb624>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscraper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-8f3e2c4b155c>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, charity_proxy)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m                         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m                             \u001b[0mpage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msearch_url\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m                         \u001b[1;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m                             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, url, **kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GET'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    502\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    503\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    486\u001b[0m         }\n\u001b[0;32m    487\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;31m# Resolve redirects if allowed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;31m# Shuffle things around if there's history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    629\u001b[0m         \u001b[1;31m# Resolve redirects if allowed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mresp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mallow_redirects\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;31m# Shuffle things around if there's history.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mresolve_redirects\u001b[1;34m(self, resp, req, stream, timeout, verify, cert, proxies, **adapter_kwargs)\u001b[0m\n\u001b[0;32m    188\u001b[0m                 \u001b[0mproxies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mproxies\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m                 \u001b[0mallow_redirects\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m                 \u001b[1;33m**\u001b[0m\u001b[0madapter_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m             )\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[1;31m# Send the request\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 609\u001b[1;33m         \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m         \u001b[1;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\requests\\adapters.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreason\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_ProxyError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mProxyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProxyError\u001b[0m: HTTPSConnectionPool(host='sfbay.craigslist.org', port=443): Max retries exceeded with url: /search/roo (Caused by ProxyError('Cannot connect to proxy.', OSError('Tunnel connection failed: 407 Proxy authentication required',)))"
     ]
    }
   ],
   "source": [
    "scraper.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attrs = tree.xpath('/html/body/section/section/section/div[1]/p[2]/span')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(attrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "laundry = any(['laundry' in attr.text for attr in attrs])\n",
    "washDry = any(['w/d' in attr.text for attr in attrs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "laundry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for attr in attrs:\n",
    "    print(attr.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#MUST FIGURE OUT HOW TO GET BEDROOMS\n",
    "#Took out baths from original code because baths are in different location for shared\n",
    "#What is row[2] in item_url = domain.split('/search')[0] + row[2]\n",
    "#logging not working\n",
    "#Add back in other functions\n",
    "    \n",
    "    \n",
    "    \n",
    "    ]\n",
    "    def PageAttributes(self, session, url, proxy=True):\n",
    "    \n",
    "        s = session\n",
    "        \n",
    "        page = s.get(url, timeout=30)        \n",
    "        tree = html.fromstring(page.content)\n",
    "        try:\n",
    "            pageattrs = tree.xpath('//div[@class=\"mapAndAttrs\"]/p[@class=\"attrgroup\"]/span/b')\n",
    "        except:\n",
    "            pageattrs = []\n",
    "\n",
    "        return pageattrs\n",
    "    \n",
    "        if 'private room' in pageattrs:\n",
    "            private_room = True \n",
    "   \n",
    "        if 'private bath' in pageattrs:\n",
    "            private_bath = True\n",
    "            \n",
    "        parking=['carport','attached garage','detached garage']\n",
    "        if any(i in parking for i in pageattrs):\n",
    "            carport_or_garage = True\n",
    "\n",
    "        if 'w/d in unit' in pageattrs:\n",
    "            washer_unit = True\n",
    "            \n",
    "        washer_list=['laundry in bldg','laundry on site']\n",
    "        if any(i in washer_list for i in pageattrs):\n",
    "            washer_building = True\n",
    "      \n",
    "        return [private_room, private_bath, carport_or_garage, washer_unit, washer_building]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first = ([True, False, False, True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "second = ([False, False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "first or second"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
