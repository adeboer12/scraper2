{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "import importlib\n",
    "import urllib\n",
    "import unicodecsv as csv\n",
    "from lxml import html\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt, numpy as np\n",
    "import json\n",
    "import sys\n",
    "from requests.auth import HTTPProxyAuth\n",
    "import time\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Script is written right now for before cleaned, single region. Must adapt for cleaned and multi-region.\n",
    "#This will include changing \"price\" to \"rent\"\n",
    "#QUESTION: Unclear what is actually being counted in 'score' variable\n",
    "\n",
    "#cleandf = pd.read_csv('C:\\\\Users\\\\james\\\\Documents\\\\Berkeley_Docs\\\\Spring_17_Courses\\\\CP290 Data Lab\\\\github\\\\scraper2\\\\shared_room_scraper\\\\sfbay_merged_2017_04_10.csv', header=0) #just SF dataframe\n",
    "df = pd.read_csv('C:\\\\Users\\\\james\\\\Documents\\\\Berkeley_Docs\\\\Spring_17_Courses\\\\CP290 Data Lab\\\\github\\\\scraper2\\\\shared_room_scraper\\\\all_data_merged_2017_04_six_nine.csv',error_bad_lines=False, header=0) #all listings dataframe\n",
    "#determine rent per square foot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[df.lat == 99, 'lat'] = 0\n",
    "df.loc[df.lng == 99, 'lng'] = 0\n",
    "df.loc[df.sqft == 0, 'sqft'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For any duplicate post, we want to keep the version with the most information. Therefore, assign a score to each post and \n",
    "#give one point for square footage, lat, long and price\n",
    "#OLD FORMULA\n",
    "#cleandf['score']=cleandf.astype(bool).sum(axis=1)\n",
    "\n",
    "df['price_exists'] = df['price']>0\n",
    "df['sqft_exists'] = df['sqft']>0\n",
    "df['lat_exists'] = df['lat']>0\n",
    "df['lng_exists'] = df['lng']<0\n",
    "df['score'] = df[['price_exists','sqft_exists','lat_exists','lng_exists']].astype(bool).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sort rows by score\n",
    "df = df.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List as 'true' if the PID, body_text, or the title are duplicates\n",
    "df['body_text_duplicate']=df.duplicated('body_text')\n",
    "df['title_duplicate']=df.duplicated('title')\n",
    "df['PID_duplicate']=df.duplicated('pid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dedupe1 = df.drop_duplicates('pid')\n",
    "df_dedupe1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep rows with highest score. Drop rows with duplicate PIDs, Duplicate Titles, Duplicate Posting Bodies\n",
    "df_dedupe2 = df_dedupe1.drop_duplicates('body_text')\n",
    "df_dedupe2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique = df_dedupe2.drop_duplicates('title')\n",
    "df_unique.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick bar chart to see impact of deduplication on sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [len(df),len(df_dedupe1), len(df_dedupe2), len(df_unique)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['original', 'dedupe-pid', 'dedupe-body', 'dedupe-title']\n",
    "x = [1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6) )\n",
    "plt.suptitle('Deduplication Sample Size')\n",
    "plt.xlabel('Deduplication Phase')\n",
    "plt.ylabel('Number of Listings')\n",
    "plt.xticks(x, labels)\n",
    "ax = plt.bar(x, y, alpha=.4, color='cyan', align='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new fields for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique['dt'] = pd.to_datetime(df_unique['dt'], format='%Y-%m-%d')\n",
    "df_unique['region'] = df_unique['url'].str.extract('http://(.*).craigslist.org', expand=False)\n",
    "df_unique['day_of_week'] = df_unique['dt'].apply(lambda x: x.weekday())\n",
    "df_unique['rent_sqft'] = df_unique['price'] / df_unique['sqft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique['date'] = pd.DatetimeIndex(df_unique.dt).normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#counting listings per day (right now inlcudes time, need to remove)\n",
    "listings_per_date = df_unique['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_per_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Stats and More Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_unique.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_wprice = df_unique[df_unique['price'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this cell, define the values by which we will filter the 3 columns. This will vary depending on the sample we're looking at. \n",
    "upper_percentile = 0.9965\n",
    "lower_percentile = 0.03\n",
    "\n",
    "# how many rows would be within the upper and lower percentiles?\n",
    "upper = int(len(unique_wprice) * upper_percentile)\n",
    "lower = int(len(unique_wprice) * lower_percentile)\n",
    "\n",
    "# get the rent/sqft values at the upper and lower percentiles\n",
    "rent_sqft_sorted = unique_wprice['rent_sqft'].sort_values(ascending=True, inplace=False)\n",
    "upper_rent_sqft = rent_sqft_sorted.iloc[upper]\n",
    "lower_rent_sqft = rent_sqft_sorted.iloc[lower]\n",
    "\n",
    "# get the rent values at the upper and lower percentiles\n",
    "rent_sorted = unique_wprice['price'].sort_values(ascending=True, inplace=False)\n",
    "upper_rent = rent_sorted.iloc[upper]\n",
    "lower_rent = rent_sorted.iloc[lower]\n",
    "\n",
    "# get the sqft values at the upper and lower percentiles\n",
    "sqft_sorted = unique_wprice['sqft'].sort_values(ascending=True, inplace=False)\n",
    "upper_sqft = sqft_sorted.iloc[upper]\n",
    "lower_sqft = sqft_sorted.iloc[lower]\n",
    "\n",
    "print('valid rent_sqft range:', [lower_rent_sqft, upper_rent_sqft])\n",
    "print('valid rent range:', [lower_rent, upper_rent])\n",
    "print('valid sqft range:', [lower_sqft, upper_sqft])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out rows with unreasonable rent prices \n",
    "rent_mask = (unique_wprice['price'] > lower_rent) & (unique_wprice['price'] < upper_rent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_listings = pd.DataFrame(unique_wprice[rent_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering out listings with \"0\" latitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lat_mask = (filtered_listings['lat'] > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_listings = pd.DataFrame(filtered_listings[lat_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered_listings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting FIPS (Need to add to Istanbul script, too slow here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creating columns for FIPS\n",
    "filtered_listings['FIPS'] = None\n",
    "filtered_listings['state'] = None\n",
    "filtered_listings['county'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each row to the API call\n",
    "\n",
    "for i, row in filtered_listings.iterrows():\n",
    "    url = 'http://data.fcc.gov/api/block/find?format=json&latitude={0}&longitude={1}'\n",
    "    request = url.format(row['lat'], row['lng'])\n",
    "    response = requests.get(request)\n",
    "    data = response.json()\n",
    "    filtered_listings.loc[i,['FIPS','state','county']] = [data['Block']['FIPS'], data['State']['code'], data['County']['name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[data['Block']['FIPS'], data['State']['code'], data['County']['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rooms = filtered_listings[filtered_listings['room_known']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rooms[df_rooms['private_room']==True].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rooms[df_rooms['private_room']==False].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Viewing Results by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we're pulling select regions to do neighborhood analysis, as well as intraregional comparisons to the conventional \n",
    "# listings and inter-regional comparisons. \n",
    "\n",
    "#Top 30 regions by unique postings\n",
    "filtered_listings['region'].value_counts().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nylistings = filtered_listings[filtered_listings['region']=='newyork']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = nylistings.groupby(by='neighb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counted = grouped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counted.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodNeighbs = counted['neighb'][counted['pid'] > 20].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nyneighbs = nylistings[nylistings['neighb'].isin(goodNeighbs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nyneighbs.groupby('neighb')['price'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sflistings = filtered_listings[filtered_listings['region']=='sfbay']\n",
    "sfgrouped = sflistings.groupby(by='neighb')\n",
    "sfcounted = sfgrouped.count()\n",
    "sfcounted.reset_index(inplace=True)\n",
    "sfgoodNeighbs = sfcounted['neighb'][sfcounted['pid'] > 20].values\n",
    "top_sfneighbs = sflistings[sflistings['neighb'].isin(sfgoodNeighbs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sfneighbs.groupby('neighb')['price'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = filtered_listings['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10,6) )\n",
    "plt.suptitle('Rent Price Distribution')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Number of Listings')\n",
    "ax = plt.hist(x, bins=50, alpha=.4, color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regional Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here we're pulling select regions to do neighborhood analysis, as well as intraregional comparisons to the conventional \n",
    "# listings and inter-regional comparisons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_listings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting Words in body text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(filtered_listings['body_text'].str.contains('professional'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(filtered_listings['body_text'].str.contains('young'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(filtered_listings['body_text'].str.contains('20s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(filtered_listings['body_text'].str.contains('30s'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filtered_listings.to_csv('sfbay_filtered4_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the ratios of unique to duplicate listings for each region\n",
    "listings_ratios = pd.DataFrame()\n",
    "\n",
    "# number of total listings for each region\n",
    "listings_ratios['cleandf'] = cleandf['region'].value_counts()\n",
    "\n",
    "# number of duplicate listings for each region (ie, listings that share a pid with at least one other listing)\n",
    "listings_ratios['duplicate_listings'] = duplicate_listings['region'].value_counts()\n",
    "\n",
    "# number of unique listings for the region (ie, none share a pid with another listing)\n",
    "listings_ratios['unique_listings'] = unique_listings['region'].value_counts()\n",
    "\n",
    "# percent of this region's listings that are duplicates\n",
    "listings_ratios['duplicate_ratio'] = listings_ratios['duplicate_listings'] / listings_ratios['all_listings']\n",
    "\n",
    "# percent of this region's listings that are unique (ie, not duplicates)\n",
    "listings_ratios['unique_ratio'] = listings_ratios['unique_listings'] / listings_ratios['all_listings']\n",
    "\n",
    "listings_ratios.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
