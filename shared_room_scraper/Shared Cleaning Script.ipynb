{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime as dt\n",
    "from datetime import timedelta\n",
    "import logging\n",
    "import importlib\n",
    "import urllib\n",
    "import unicodecsv as csv\n",
    "from lxml import html\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt, numpy as np\n",
    "import json\n",
    "import sys\n",
    "from requests.auth import HTTPProxyAuth\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Script is written right now for before cleaned, single region. Must adapt for cleaned and multi-region.\n",
    "#This will include changing \"price\" to \"rent\"\n",
    "#QUESTION: Unclear what is actually being counted in 'score' variable\n",
    "\n",
    "#cleandf = pd.read_csv('C:\\\\Users\\\\james\\\\Documents\\\\Berkeley_Docs\\\\Spring_17_Courses\\\\CP290 Data Lab\\\\github\\\\scraper2\\\\shared_room_scraper\\\\sfbay_merged_2017_04_10.csv', header=0) #just SF dataframe\n",
    "cleandf = pd.read_csv('C:\\\\Users\\\\james\\\\Documents\\\\Berkeley_Docs\\\\Spring_17_Courses\\\\CP290 Data Lab\\\\github\\\\scraper2\\\\shared_room_scraper\\\\all_data_merged_2017_04_06.csv',error_bad_lines=False, header=0) #all listings dataframe\n",
    "#determine rent per square foot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleandf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cleandf.loc[cleandf.lat == 99, 'lat'] = 0\n",
    "cleandf.loc[cleandf.lng == 99, 'lng'] = 0\n",
    "cleandf.loc[cleandf.sqft == 0, 'sqft'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For any duplicate post, we want to keep the version with the most information. Therefore, assign a score to each post and \n",
    "#give one point for square footage, lat, long and price\n",
    "#OLD FORMULA\n",
    "#cleandf['score']=cleandf.astype(bool).sum(axis=1)\n",
    "\n",
    "cleandf['price_exists'] = cleandf['price']>0\n",
    "cleandf['sqft_exists'] = cleandf['sqft']>0\n",
    "cleandf['lat_exists'] = cleandf['lat']>0\n",
    "cleandf['lng_exists'] = cleandf['lng']<0\n",
    "cleandf['score'] = cleandf[['price_exists','sqft_exists','lat_exists','lng_exists']].astype(bool).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sort rows by score\n",
    "cleandf = cleandf.sort_values(by='score',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleandf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleandf.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#List as 'true' if the PID, body_text, or the title are duplicates\n",
    "cleandf['body_text_duplicate']=cleandf.duplicated('body_text')\n",
    "cleandf['title_duplicate']=cleandf.duplicated('title')\n",
    "cleandf['PID_duplicate']=cleandf.duplicated('pid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleandf_unique = cleandf.drop_duplicates('pid')\n",
    "cleandf_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Keep rows with highest score. Drop rows with duplicate PIDs, Duplicate Titles, Duplicate Posting Bodies\n",
    "cleandf_unique = cleandf_unique.drop_duplicates('body_text')\n",
    "cleandf_unique.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleandf_unique = cleandf_unique.drop_duplicates('title')\n",
    "cleandf_unique.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new fields for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cleandf_unique['dt'] = pd.to_datetime(cleandf_unique['dt'], format='%Y-%m-%d')\n",
    "cleandf_unique['region'] = cleandf_unique['url'].str.extract('http://(.*).craigslist.org', expand=False)\n",
    "cleandf_unique['day_of_week'] = cleandf_unique['dt'].apply(lambda x: x.weekday())\n",
    "cleandf_unique['rent_sqft'] = cleandf_unique['price'] / cleandf['sqft']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#counting listings per day (right now inlcudes time, need to remove)\n",
    "listings_per_date = cleandf_unique['dt'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Stats and More Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleandf_unique.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_wprice = cleandf_unique[cleandf_unique['price'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# in this cell, define the values by which we will filter the 3 columns\n",
    "upper_percentile = 0.995\n",
    "lower_percentile = 0.005\n",
    "\n",
    "# how many rows would be within the upper and lower percentiles?\n",
    "upper = int(len(unique_wprice) * upper_percentile)\n",
    "lower = int(len(unique_wprice) * lower_percentile)\n",
    "\n",
    "# get the rent/sqft values at the upper and lower percentiles\n",
    "rent_sqft_sorted = unique_wprice['rent_sqft'].sort_values(ascending=True, inplace=False)\n",
    "upper_rent_sqft = rent_sqft_sorted.iloc[upper]\n",
    "lower_rent_sqft = rent_sqft_sorted.iloc[lower]\n",
    "\n",
    "# get the rent values at the upper and lower percentiles\n",
    "rent_sorted = unique_wprice['price'].sort_values(ascending=True, inplace=False)\n",
    "upper_rent = rent_sorted.iloc[upper]\n",
    "lower_rent = rent_sorted.iloc[lower]\n",
    "\n",
    "# get the sqft values at the upper and lower percentiles\n",
    "sqft_sorted = unique_wprice['sqft'].sort_values(ascending=True, inplace=False)\n",
    "upper_sqft = sqft_sorted.iloc[upper]\n",
    "lower_sqft = sqft_sorted.iloc[lower]\n",
    "\n",
    "print('valid rent_sqft range:', [lower_rent_sqft, upper_rent_sqft])\n",
    "print('valid rent range:', [lower_rent, upper_rent])\n",
    "print('valid sqft range:', [lower_sqft, upper_sqft])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Filtering out rows with unreasonable rent prices \n",
    "rent_mask = (unique_wprice['price'] > lower_rent) & (unique_wprice['price'] < upper_rent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_listings = pd.DataFrame(unique_wprice[rent_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filtered_listings.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby Function for Viewing Results by Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = cleandf_unique.groupby('region')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = filtered_listings['price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = x.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10,6), )\n",
    "plt.suptitle('Rent Price Distribution')\n",
    "plt.xlabel('Price')\n",
    "plt.ylabel('Number of Listings')\n",
    "ax = plt.hist(x, bins=50, alpha=.4, color='cyan')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filtered_listings.to_csv('sfbay_filtered4_10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate the ratios of unique to duplicate listings for each region\n",
    "listings_ratios = pd.DataFrame()\n",
    "\n",
    "# number of total listings for each region\n",
    "listings_ratios['cleandf'] = cleandf['region'].value_counts()\n",
    "\n",
    "# number of duplicate listings for each region (ie, listings that share a pid with at least one other listing)\n",
    "listings_ratios['duplicate_listings'] = duplicate_listings['region'].value_counts()\n",
    "\n",
    "# number of unique listings for the region (ie, none share a pid with another listing)\n",
    "listings_ratios['unique_listings'] = unique_listings['region'].value_counts()\n",
    "\n",
    "# percent of this region's listings that are duplicates\n",
    "listings_ratios['duplicate_ratio'] = listings_ratios['duplicate_listings'] / listings_ratios['all_listings']\n",
    "\n",
    "# percent of this region's listings that are unique (ie, not duplicates)\n",
    "listings_ratios['unique_ratio'] = listings_ratios['unique_listings'] / listings_ratios['all_listings']\n",
    "\n",
    "listings_ratios.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
